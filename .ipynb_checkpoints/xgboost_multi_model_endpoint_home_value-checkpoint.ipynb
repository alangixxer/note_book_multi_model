{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Multi-Model Endpoints using XGBoost\n",
    "With [Amazon SageMaker multi-model endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html), customers can create an endpoint that seamlessly hosts up to thousands of models. These endpoints are well suited to use cases where any one of a large number of models, which can be served from a common inference container, needs to be invokable on-demand and where it is acceptable for infrequently invoked models to incur some additional latency. For applications which require consistently low inference latency, a traditional endpoint is still the best choice.\n",
    "\n",
    "At a high level, Amazon SageMaker manages the loading and unloading of models for a multi-model endpoint, as they are needed. When an invocation request is made for a particular model, Amazon SageMaker routes the request to an instance assigned to that model, downloads the model artifacts from S3 onto that instance, and initiates loading of the model into the memory of the container. As soon as the loading is complete, Amazon SageMaker performs the requested invocation and returns the result. If the model is already loaded in memory on the selected instance, the downloading and loading steps are skipped and the invocation is performed immediately.\n",
    "\n",
    "To demonstrate how multi-model endpoints are created and used, this notebook provides an example using a set of XGBoost models that each predict housing prices for a single location. This domain is used as a simple example to easily experiment with multi-model endpoints.\n",
    "\n",
    "The Amazon SageMaker multi-model endpoint capability is designed to work across all machine learning frameworks and algorithms including those where you bring your own container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "1. [Build and register an XGBoost container that can serve multiple models](#Build-and-register-an-XGBoost-container-that-can-serve-multiple-models)\n",
    "1. [Generate synthetic data for housing models](#Generate-synthetic-data-for-housing-models)\n",
    "1. [Train multiple house value prediction models](#Train-multiple-house-value-prediction-models)\n",
    "1. [Import models into hosting](#Import-models-into-hosting)\n",
    "  1. [Deploy model artifacts to be found by the endpoint](#Deploy-model-artifacts-to-be-found-by-the-endpoint)\n",
    "  1. [Create the Amazon SageMaker model entity](#Create-the-Amazon-SageMaker-model-entity)\n",
    "  1. [Create the multi-model endpoint](#Create-the-multi-model-endpoint)\n",
    "1. [Exercise the multi-model endpoint](#Exercise-the-multi-model-endpoint)\n",
    "  1. [Dynamically deploy another model](#Dynamically-deploy-another-model)\n",
    "  1. [Invoke the newly deployed model](#Invoke-the-newly-deployed-model)\n",
    "  1. [Updating a model](#Updating-a-model)\n",
    "1. [Clean up](#Clean-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and register an XGBoost container that can serve multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU awscli boto3 sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the inference container to serve multiple models in a multi-model endpoint, it must implement [additional APIs](https://docs.aws.amazon.com/sagemaker/latest/dg/build-multi-model-build-container.html) in order to load, list, get, unload and invoke specific models.\n",
    "\n",
    "The ['mme' branch of the SageMaker XGBoost Container repository](https://github.com/aws/sagemaker-xgboost-container/tree/mme) is an example implementation on how to adapt SageMaker's XGBoost framework container to use [Multi Model Server](https://github.com/awslabs/multi-model-server), a framework that provides an HTTP frontend that implements the additional container APIs required by multi-model endpoints, and also provides a pluggable backend handler for serving models using a custom framework, in this case the XGBoost framework.\n",
    "\n",
    "Using this branch, below we will build an XGBoost container that fulfills all of the multi-model endpoint container requirements, and then upload that image to Amazon Elastic Container Registry (ECR). Because uploading the image to ECR may create a new ECR repository, this notebook requires permissions in addition to the regular SageMakerFullAccess permissions. The easiest way to add these permissions is simply to add the managed policy AmazonEC2ContainerRegistryFullAccess to the role that you used to start your notebook instance. There's no need to restart your notebook instance when you do this, the new permissions will be available immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHM_NAME = 'multi-model-xgboost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "sha256:fcaf910662066f7d118d6d7f50ff6d7399d189611a5f5a8772123ce74c15cdff\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "creating build/lib/sagemaker_xgboost_container\n",
      "copying src/sagemaker_xgboost_container/distributed.py -> build/lib/sagemaker_xgboost_container\n",
      "copying src/sagemaker_xgboost_container/encoder.py -> build/lib/sagemaker_xgboost_container\n",
      "copying src/sagemaker_xgboost_container/serving.py -> build/lib/sagemaker_xgboost_container\n",
      "copying src/sagemaker_xgboost_container/__init__.py -> build/lib/sagemaker_xgboost_container\n",
      "copying src/sagemaker_xgboost_container/checkpointing.py -> build/lib/sagemaker_xgboost_container\n",
      "copying src/sagemaker_xgboost_container/training.py -> build/lib/sagemaker_xgboost_container\n",
      "copying src/sagemaker_xgboost_container/handler_service.py -> build/lib/sagemaker_xgboost_container\n",
      "copying src/sagemaker_xgboost_container/data_utils.py -> build/lib/sagemaker_xgboost_container\n",
      "creating build/lib/sagemaker_algorithm_toolkit\n",
      "copying src/sagemaker_algorithm_toolkit/__init__.py -> build/lib/sagemaker_algorithm_toolkit\n",
      "copying src/sagemaker_algorithm_toolkit/channel_validation.py -> build/lib/sagemaker_algorithm_toolkit\n",
      "copying src/sagemaker_algorithm_toolkit/metrics.py -> build/lib/sagemaker_algorithm_toolkit\n",
      "copying src/sagemaker_algorithm_toolkit/exceptions.py -> build/lib/sagemaker_algorithm_toolkit\n",
      "copying src/sagemaker_algorithm_toolkit/hyperparameter_validation.py -> build/lib/sagemaker_algorithm_toolkit\n",
      "copying src/sagemaker_algorithm_toolkit/metadata.py -> build/lib/sagemaker_algorithm_toolkit\n",
      "creating build/lib/sagemaker_xgboost_container/metrics\n",
      "copying src/sagemaker_xgboost_container/metrics/__init__.py -> build/lib/sagemaker_xgboost_container/metrics\n",
      "copying src/sagemaker_xgboost_container/metrics/custom_metrics.py -> build/lib/sagemaker_xgboost_container/metrics\n",
      "creating build/lib/sagemaker_xgboost_container/dmlc_patch\n",
      "copying src/sagemaker_xgboost_container/dmlc_patch/tracker.py -> build/lib/sagemaker_xgboost_container/dmlc_patch\n",
      "copying src/sagemaker_xgboost_container/dmlc_patch/__init__.py -> build/lib/sagemaker_xgboost_container/dmlc_patch\n",
      "creating build/lib/sagemaker_xgboost_container/algorithm_mode\n",
      "copying src/sagemaker_xgboost_container/algorithm_mode/integration.py -> build/lib/sagemaker_xgboost_container/algorithm_mode\n",
      "copying src/sagemaker_xgboost_container/algorithm_mode/__init__.py -> build/lib/sagemaker_xgboost_container/algorithm_mode\n",
      "copying src/sagemaker_xgboost_container/algorithm_mode/train.py -> build/lib/sagemaker_xgboost_container/algorithm_mode\n",
      "copying src/sagemaker_xgboost_container/algorithm_mode/channel_validation.py -> build/lib/sagemaker_xgboost_container/algorithm_mode\n",
      "copying src/sagemaker_xgboost_container/algorithm_mode/inference_errors.py -> build/lib/sagemaker_xgboost_container/algorithm_mode\n",
      "copying src/sagemaker_xgboost_container/algorithm_mode/metrics.py -> build/lib/sagemaker_xgboost_container/algorithm_mode\n",
      "copying src/sagemaker_xgboost_container/algorithm_mode/train_utils.py -> build/lib/sagemaker_xgboost_container/algorithm_mode\n",
      "copying src/sagemaker_xgboost_container/algorithm_mode/handler_service.py -> build/lib/sagemaker_xgboost_container/algorithm_mode\n",
      "copying src/sagemaker_xgboost_container/algorithm_mode/hyperparameter_validation.py -> build/lib/sagemaker_xgboost_container/algorithm_mode\n",
      "copying src/sagemaker_xgboost_container/algorithm_mode/metadata.py -> build/lib/sagemaker_xgboost_container/algorithm_mode\n",
      "creating build/lib/sagemaker_xgboost_container/mms_patch\n",
      "copying src/sagemaker_xgboost_container/mms_patch/__init__.py -> build/lib/sagemaker_xgboost_container/mms_patch\n",
      "copying src/sagemaker_xgboost_container/mms_patch/mms_transformer.py -> build/lib/sagemaker_xgboost_container/mms_patch\n",
      "copying src/sagemaker_xgboost_container/mms_patch/model_server.py -> build/lib/sagemaker_xgboost_container/mms_patch\n",
      "creating build/lib/sagemaker_xgboost_container/constants\n",
      "copying src/sagemaker_xgboost_container/constants/xgb_constants.py -> build/lib/sagemaker_xgboost_container/constants\n",
      "copying src/sagemaker_xgboost_container/constants/xgb_content_types.py -> build/lib/sagemaker_xgboost_container/constants\n",
      "copying src/sagemaker_xgboost_container/constants/__init__.py -> build/lib/sagemaker_xgboost_container/constants\n",
      "copying src/sagemaker_xgboost_container/constants/sm_env_constants.py -> build/lib/sagemaker_xgboost_container/constants\n",
      "installing to build/bdist.linux-x86_64/wheel\n",
      "running install\n",
      "running install_lib\n",
      "creating build/bdist.linux-x86_64\n",
      "creating build/bdist.linux-x86_64/wheel\n",
      "creating build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container\n",
      "copying build/lib/sagemaker_xgboost_container/distributed.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container\n",
      "copying build/lib/sagemaker_xgboost_container/encoder.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container\n",
      "copying build/lib/sagemaker_xgboost_container/serving.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container\n",
      "copying build/lib/sagemaker_xgboost_container/__init__.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container\n",
      "creating build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/metrics\n",
      "copying build/lib/sagemaker_xgboost_container/metrics/__init__.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/metrics\n",
      "copying build/lib/sagemaker_xgboost_container/metrics/custom_metrics.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/metrics\n",
      "creating build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/dmlc_patch\n",
      "copying build/lib/sagemaker_xgboost_container/dmlc_patch/tracker.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/dmlc_patch\n",
      "copying build/lib/sagemaker_xgboost_container/dmlc_patch/__init__.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/dmlc_patch\n",
      "creating build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/algorithm_mode\n",
      "copying build/lib/sagemaker_xgboost_container/algorithm_mode/integration.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/algorithm_mode\n",
      "copying build/lib/sagemaker_xgboost_container/algorithm_mode/__init__.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/algorithm_mode\n",
      "copying build/lib/sagemaker_xgboost_container/algorithm_mode/train.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/algorithm_mode\n",
      "copying build/lib/sagemaker_xgboost_container/algorithm_mode/channel_validation.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/algorithm_mode\n",
      "copying build/lib/sagemaker_xgboost_container/algorithm_mode/inference_errors.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/algorithm_mode\n",
      "copying build/lib/sagemaker_xgboost_container/algorithm_mode/metrics.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/algorithm_mode\n",
      "copying build/lib/sagemaker_xgboost_container/algorithm_mode/train_utils.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/algorithm_mode\n",
      "copying build/lib/sagemaker_xgboost_container/algorithm_mode/handler_service.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/algorithm_mode\n",
      "copying build/lib/sagemaker_xgboost_container/algorithm_mode/hyperparameter_validation.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/algorithm_mode\n",
      "copying build/lib/sagemaker_xgboost_container/algorithm_mode/metadata.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/algorithm_mode\n",
      "copying build/lib/sagemaker_xgboost_container/checkpointing.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container\n",
      "copying build/lib/sagemaker_xgboost_container/training.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container\n",
      "copying build/lib/sagemaker_xgboost_container/handler_service.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container\n",
      "creating build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/mms_patch\n",
      "copying build/lib/sagemaker_xgboost_container/mms_patch/__init__.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/mms_patch\n",
      "copying build/lib/sagemaker_xgboost_container/mms_patch/mms_transformer.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/mms_patch\n",
      "copying build/lib/sagemaker_xgboost_container/mms_patch/model_server.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/mms_patch\n",
      "creating build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/constants\n",
      "copying build/lib/sagemaker_xgboost_container/constants/xgb_constants.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/constants\n",
      "copying build/lib/sagemaker_xgboost_container/constants/xgb_content_types.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/constants\n",
      "copying build/lib/sagemaker_xgboost_container/constants/__init__.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/constants\n",
      "copying build/lib/sagemaker_xgboost_container/constants/sm_env_constants.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container/constants\n",
      "copying build/lib/sagemaker_xgboost_container/data_utils.py -> build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container\n",
      "creating build/bdist.linux-x86_64/wheel/sagemaker_algorithm_toolkit\n",
      "copying build/lib/sagemaker_algorithm_toolkit/__init__.py -> build/bdist.linux-x86_64/wheel/sagemaker_algorithm_toolkit\n",
      "copying build/lib/sagemaker_algorithm_toolkit/channel_validation.py -> build/bdist.linux-x86_64/wheel/sagemaker_algorithm_toolkit\n",
      "copying build/lib/sagemaker_algorithm_toolkit/metrics.py -> build/bdist.linux-x86_64/wheel/sagemaker_algorithm_toolkit\n",
      "copying build/lib/sagemaker_algorithm_toolkit/exceptions.py -> build/bdist.linux-x86_64/wheel/sagemaker_algorithm_toolkit\n",
      "copying build/lib/sagemaker_algorithm_toolkit/hyperparameter_validation.py -> build/bdist.linux-x86_64/wheel/sagemaker_algorithm_toolkit\n",
      "copying build/lib/sagemaker_algorithm_toolkit/metadata.py -> build/bdist.linux-x86_64/wheel/sagemaker_algorithm_toolkit\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating src/sagemaker_xgboost_container.egg-info\n",
      "writing src/sagemaker_xgboost_container.egg-info/PKG-INFO\n",
      "writing dependency_links to src/sagemaker_xgboost_container.egg-info/dependency_links.txt\n",
      "writing entry points to src/sagemaker_xgboost_container.egg-info/entry_points.txt\n",
      "writing requirements to src/sagemaker_xgboost_container.egg-info/requires.txt\n",
      "writing top-level names to src/sagemaker_xgboost_container.egg-info/top_level.txt\n",
      "writing manifest file 'src/sagemaker_xgboost_container.egg-info/SOURCES.txt'\n",
      "reading manifest file 'src/sagemaker_xgboost_container.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "writing manifest file 'src/sagemaker_xgboost_container.egg-info/SOURCES.txt'\n",
      "Copying src/sagemaker_xgboost_container.egg-info to build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container-1.0-py3.6.egg-info\n",
      "running install_scripts\n",
      "creating build/bdist.linux-x86_64/wheel/sagemaker_xgboost_container-1.0.dist-info/WHEEL\n",
      "creating '/home/ec2-user/SageMaker/multi_model_xgboost_home_value_2020-04-15/sagemaker-xgboost-container/dist/sagemaker_xgboost_container-1.0-py2.py3-none-any.whl' and adding '.' to it\n",
      "adding 'sagemaker_algorithm_toolkit/__init__.py'\n",
      "adding 'sagemaker_algorithm_toolkit/channel_validation.py'\n",
      "adding 'sagemaker_algorithm_toolkit/exceptions.py'\n",
      "adding 'sagemaker_algorithm_toolkit/hyperparameter_validation.py'\n",
      "adding 'sagemaker_algorithm_toolkit/metadata.py'\n",
      "adding 'sagemaker_algorithm_toolkit/metrics.py'\n",
      "adding 'sagemaker_xgboost_container/__init__.py'\n",
      "adding 'sagemaker_xgboost_container/checkpointing.py'\n",
      "adding 'sagemaker_xgboost_container/data_utils.py'\n",
      "adding 'sagemaker_xgboost_container/distributed.py'\n",
      "adding 'sagemaker_xgboost_container/encoder.py'\n",
      "adding 'sagemaker_xgboost_container/handler_service.py'\n",
      "adding 'sagemaker_xgboost_container/serving.py'\n",
      "adding 'sagemaker_xgboost_container/training.py'\n",
      "adding 'sagemaker_xgboost_container/algorithm_mode/__init__.py'\n",
      "adding 'sagemaker_xgboost_container/algorithm_mode/channel_validation.py'\n",
      "adding 'sagemaker_xgboost_container/algorithm_mode/handler_service.py'\n",
      "adding 'sagemaker_xgboost_container/algorithm_mode/hyperparameter_validation.py'\n",
      "adding 'sagemaker_xgboost_container/algorithm_mode/inference_errors.py'\n",
      "adding 'sagemaker_xgboost_container/algorithm_mode/integration.py'\n",
      "adding 'sagemaker_xgboost_container/algorithm_mode/metadata.py'\n",
      "adding 'sagemaker_xgboost_container/algorithm_mode/metrics.py'\n",
      "adding 'sagemaker_xgboost_container/algorithm_mode/train.py'\n",
      "adding 'sagemaker_xgboost_container/algorithm_mode/train_utils.py'\n",
      "adding 'sagemaker_xgboost_container/constants/__init__.py'\n",
      "adding 'sagemaker_xgboost_container/constants/sm_env_constants.py'\n",
      "adding 'sagemaker_xgboost_container/constants/xgb_constants.py'\n",
      "adding 'sagemaker_xgboost_container/constants/xgb_content_types.py'\n",
      "adding 'sagemaker_xgboost_container/dmlc_patch/__init__.py'\n",
      "adding 'sagemaker_xgboost_container/dmlc_patch/tracker.py'\n",
      "adding 'sagemaker_xgboost_container/metrics/__init__.py'\n",
      "adding 'sagemaker_xgboost_container/metrics/custom_metrics.py'\n",
      "adding 'sagemaker_xgboost_container/mms_patch/__init__.py'\n",
      "adding 'sagemaker_xgboost_container/mms_patch/mms_transformer.py'\n",
      "adding 'sagemaker_xgboost_container/mms_patch/model_server.py'\n",
      "adding 'sagemaker_xgboost_container-1.0.dist-info/entry_points.txt'\n",
      "adding 'sagemaker_xgboost_container-1.0.dist-info/top_level.txt'\n",
      "adding 'sagemaker_xgboost_container-1.0.dist-info/WHEEL'\n",
      "adding 'sagemaker_xgboost_container-1.0.dist-info/METADATA'\n",
      "adding 'sagemaker_xgboost_container-1.0.dist-info/RECORD'\n",
      "removing build/bdist.linux-x86_64/wheel\n",
      "sha256:7b60d0250adc568c5fb7ba25f6eaa29607f7b847574d86d620eb9d7a7a3a9c4a\n",
      "The push refers to repository [111016121260.dkr.ecr.us-east-2.amazonaws.com/multi-model-xgboost]\n",
      "d805c3df3eef: Preparing\n",
      "2a5837fde731: Preparing\n",
      "b181fb463b7f: Preparing\n",
      "9ed4ac7a505e: Preparing\n",
      "5fa7a4fde7c0: Preparing\n",
      "1007241395c6: Preparing\n",
      "b553351056a8: Preparing\n",
      "ab67c1485779: Preparing\n",
      "735ade3f924c: Preparing\n",
      "09d339fe9303: Preparing\n",
      "9b371d7d065d: Preparing\n",
      "651ca1d36f76: Preparing\n",
      "104870213070: Preparing\n",
      "d4ca4fe3983a: Preparing\n",
      "e464635197fb: Preparing\n",
      "6bd851ac3e44: Preparing\n",
      "60db65ccb87d: Preparing\n",
      "bf9c2c582d34: Preparing\n",
      "735ade3f924c: Waiting\n",
      "49b7712f6f43: Preparing\n",
      "b553351056a8: Waiting\n",
      "2b7b1b6f5804: Preparing\n",
      "321ef45b0f72: Preparing\n",
      "09d339fe9303: Waiting\n",
      "ab67c1485779: Waiting\n",
      "2d2a4b1da475: Preparing\n",
      "30cbd691867b: Preparing\n",
      "4ae3adcb66cb: Preparing\n",
      "aa6685385151: Preparing\n",
      "0040d8f00d7e: Preparing\n",
      "9e6f810a2aab: Preparing\n",
      "9b371d7d065d: Waiting\n",
      "651ca1d36f76: Waiting\n",
      "104870213070: Waiting\n",
      "d4ca4fe3983a: Waiting\n",
      "e464635197fb: Waiting\n",
      "6bd851ac3e44: Waiting\n",
      "60db65ccb87d: Waiting\n",
      "bf9c2c582d34: Waiting\n",
      "49b7712f6f43: Waiting\n",
      "2b7b1b6f5804: Waiting\n",
      "321ef45b0f72: Waiting\n",
      "1007241395c6: Waiting\n",
      "0040d8f00d7e: Waiting\n",
      "2d2a4b1da475: Waiting\n",
      "9e6f810a2aab: Waiting\n",
      "30cbd691867b: Waiting\n",
      "aa6685385151: Waiting\n",
      "4ae3adcb66cb: Waiting\n",
      "5fa7a4fde7c0: Pushed\n",
      "2a5837fde731: Pushed\n",
      "9ed4ac7a505e: Pushed\n",
      "d805c3df3eef: Pushed\n",
      "b181fb463b7f: Pushed\n",
      "ab67c1485779: Pushed\n",
      "b553351056a8: Pushed\n",
      "1007241395c6: Pushed\n",
      "735ade3f924c: Pushed\n",
      "104870213070: Layer already exists\n",
      "d4ca4fe3983a: Layer already exists\n",
      "e464635197fb: Layer already exists\n",
      "09d339fe9303: Pushed\n",
      "6bd851ac3e44: Layer already exists\n",
      "60db65ccb87d: Layer already exists\n",
      "bf9c2c582d34: Layer already exists\n",
      "49b7712f6f43: Layer already exists\n",
      "2b7b1b6f5804: Layer already exists\n",
      "321ef45b0f72: Layer already exists\n",
      "2d2a4b1da475: Layer already exists\n",
      "30cbd691867b: Layer already exists\n",
      "4ae3adcb66cb: Layer already exists\n",
      "aa6685385151: Layer already exists\n",
      "9e6f810a2aab: Layer already exists\n",
      "0040d8f00d7e: Layer already exists\n",
      "9b371d7d065d: Pushed\n",
      "651ca1d36f76: Pushed\n",
      "latest: digest: sha256:ed2821b40fd67bd7a39ab0e8fcd48e2cda056cbd9ff98f03161af44938a7f286 size: 5970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Cloning into 'sagemaker-xgboost-container'...\n"
     ]
    }
   ],
   "source": [
    "%%sh -s $ALGORITHM_NAME\n",
    "\n",
    "algorithm_name=$1\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration\n",
    "region=$(aws configure get region)\n",
    "\n",
    "ecr_image=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email --registry-ids ${account})\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "# First clear out any prior version of the cloned repo\n",
    "rm -rf sagemaker-xgboost-container/\n",
    "\n",
    "# Clone the xgboost container repo\n",
    "git clone --single-branch --branch mme https://github.com/aws/sagemaker-xgboost-container.git\n",
    "cd sagemaker-xgboost-container/\n",
    "\n",
    "# Build the \"base\" container image that encompasses the installation of the\n",
    "# XGBoost framework and all of the dependencies needed.\n",
    "docker build -q -t xgboost-container-base:0.90-2-cpu-py3 -f docker/0.90-2/base/Dockerfile.cpu .\n",
    "\n",
    "# Create the SageMaker XGBoost Container Python package.\n",
    "python setup.py bdist_wheel --universal\n",
    "\n",
    "# Build the \"final\" container image that encompasses the installation of the\n",
    "# code that implements the SageMaker multi-model container requirements.\n",
    "docker build -q -t ${algorithm_name} -f docker/0.90-2/final/Dockerfile.cpu .\n",
    "\n",
    "docker tag ${algorithm_name} ${ecr_image}\n",
    "\n",
    "docker push ${ecr_image}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic data for housing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datapackage in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.13.0)\n",
      "Requirement already satisfied: jsonpointer>=1.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datapackage) (2.0)\n",
      "Requirement already satisfied: unicodecsv>=0.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datapackage) (0.14.1)\n",
      "Requirement already satisfied: jsonschema>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datapackage) (2.6.0)\n",
      "Requirement already satisfied: tabulator>=1.29 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datapackage) (1.38.2)\n",
      "Requirement already satisfied: chardet>=3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datapackage) (3.0.4)\n",
      "Requirement already satisfied: click>=6.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datapackage) (6.7)\n",
      "Requirement already satisfied: six>=1.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datapackage) (1.11.0)\n",
      "Requirement already satisfied: requests>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datapackage) (2.20.0)\n",
      "Requirement already satisfied: tableschema>=1.12.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from datapackage) (1.15.3)\n",
      "Requirement already satisfied: openpyxl>=2.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tabulator>=1.29->datapackage) (3.0.3)\n",
      "Requirement already satisfied: jsonlines>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tabulator>=1.29->datapackage) (1.2.0)\n",
      "Requirement already satisfied: sqlalchemy>=0.9.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tabulator>=1.29->datapackage) (1.2.11)\n",
      "Requirement already satisfied: ijson>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tabulator>=1.29->datapackage) (3.0)\n",
      "Requirement already satisfied: xlrd>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tabulator>=1.29->datapackage) (1.1.0)\n",
      "Requirement already satisfied: boto3>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tabulator>=1.29->datapackage) (1.12.39)\n",
      "Requirement already satisfied: linear-tsv>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tabulator>=1.29->datapackage) (1.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.8->datapackage) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.8->datapackage) (1.23)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests>=2.8->datapackage) (2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tableschema>=1.12.1->datapackage) (2.7.3)\n",
      "Requirement already satisfied: rfc3986>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tableschema>=1.12.1->datapackage) (1.4.0)\n",
      "Requirement already satisfied: isodate>=0.5.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tableschema>=1.12.1->datapackage) (0.6.0)\n",
      "Requirement already satisfied: jdcal in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from openpyxl>=2.6->tabulator>=1.29->datapackage) (1.4)\n",
      "Requirement already satisfied: et-xmlfile in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from openpyxl>=2.6->tabulator>=1.29->datapackage) (1.0.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.9->tabulator>=1.29->datapackage) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.9->tabulator>=1.29->datapackage) (1.15.39)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.9->tabulator>=1.29->datapackage) (0.3.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3>=1.9->tabulator>=1.29->datapackage) (0.14)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datapackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['validation_report', 'world-cities_csv', 'world-cities_json', 'world-cities_zip', 'world-cities_csv_preview', 'world-cities']\n",
      "['Boardman_Ohio', 'Melrose_Massachusetts', 'Belmont_California', 'Bedford_Texas', 'Louisville_Kentucky', 'Bellingham_Washington', 'Kailua_Hawaii', 'MiddleRiver_Maryland', 'Hawthorne_NewJersey', 'EastPeoria_Illinois']\n"
     ]
    }
   ],
   "source": [
    "from datapackage import Package\n",
    "import random\n",
    "\n",
    "package = Package('https://datahub.io/core/world-cities/datapackage.json')\n",
    "\n",
    "# print list of all resources:\n",
    "print(package.resource_names)\n",
    "\n",
    "limit = 10\n",
    "usa_list = []\n",
    "city_list = []\n",
    "\n",
    "# print processed tabular data (if exists any)\n",
    "for resource in package.resources:\n",
    "    if resource.descriptor['datahub']['type'] == 'derived/csv':\n",
    "        for each in resource.read():\n",
    "            if each[1] == 'United States':\n",
    "                usa_list.append((each[0]+'_'+each[2]).replace(' ','').replace(\"'\",''))\n",
    "                \n",
    "city_list = random.choices(usa_list, k=limit)  \n",
    "print(city_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HOUSES_PER_LOCATION = 1000\n",
    "#LOCATIONS  = ['NewYork_NY',    'LosAngeles_CA',   'Chicago_IL',    'Houston_TX',   'Dallas_TX',\n",
    "#              'Phoenix_AZ',    'Philadelphia_PA', 'SanAntonio_TX', 'SanDiego_CA',  'SanFrancisco_CA']\n",
    "\n",
    "LOCATIONS = city_list\n",
    "PARALLEL_TRAINING_JOBS = 10 # len(LOCATIONS) if your account limits can handle it\n",
    "MAX_YEAR = 2019\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_price(house):\n",
    "    _base_price = int(house['SQUARE_FEET'] * 150)\n",
    "    _price = int(_base_price + (10000 * house['NUM_BEDROOMS']) + \\\n",
    "                               (15000 * house['NUM_BATHROOMS']) + \\\n",
    "                               (15000 * house['LOT_ACRES']) + \\\n",
    "                               (15000 * house['GARAGE_SPACES']) - \\\n",
    "                               (5000 * (MAX_YEAR - house['YEAR_BUILT'])))\n",
    "    return _price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_house():\n",
    "    _house = {'SQUARE_FEET':   int(np.random.normal(3000, 750)),\n",
    "              'NUM_BEDROOMS':  np.random.randint(2, 7),\n",
    "              'NUM_BATHROOMS': np.random.randint(2, 7) / 2,\n",
    "              'LOT_ACRES':     round(np.random.normal(1.0, 0.25), 2),\n",
    "              'GARAGE_SPACES': np.random.randint(0, 4),\n",
    "              'YEAR_BUILT':    min(MAX_YEAR, int(np.random.normal(1995, 10)))}\n",
    "    _price = gen_price(_house)\n",
    "    return [_price, _house['YEAR_BUILT'],   _house['SQUARE_FEET'], \n",
    "                    _house['NUM_BEDROOMS'], _house['NUM_BATHROOMS'], \n",
    "                    _house['LOT_ACRES'],    _house['GARAGE_SPACES']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_houses(num_houses):\n",
    "    _house_list = []\n",
    "    for i in range(num_houses):\n",
    "        _house_list.append(gen_random_house())\n",
    "    _df = pd.DataFrame(_house_list, \n",
    "                       columns=['PRICE',        'YEAR_BUILT',    'SQUARE_FEET',  'NUM_BEDROOMS',\n",
    "                                'NUM_BATHROOMS','LOT_ACRES',     'GARAGE_SPACES'])\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train multiple house value prediction models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer\n",
    "import boto3\n",
    "\n",
    "sm_client = boto3.client(service_name='sagemaker')\n",
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime')\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "ACCOUNT_ID = boto3.client('sts').get_caller_identity()['Account']\n",
    "REGION     = boto3.Session().region_name\n",
    "BUCKET     = sagemaker_session.default_bucket()\n",
    "\n",
    "MULTI_MODEL_XGBOOST_IMAGE = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(ACCOUNT_ID, REGION, \n",
    "                                                                           ALGORITHM_NAME)\n",
    "\n",
    "DATA_PREFIX            = 'DEMO_MME_XGBOOST'\n",
    "HOUSING_MODEL_NAME     = 'housing'\n",
    "MULTI_MODEL_ARTIFACTS  = 'multi_model_artifacts'\n",
    "\n",
    "TRAIN_INSTANCE_TYPE    = 'ml.m4.xlarge'\n",
    "ENDPOINT_INSTANCE_TYPE = 'ml.m4.xlarge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split a given dataset into train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "SEED = 7\n",
    "SPLIT_RATIOS = [0.6, 0.3, 0.1]\n",
    "\n",
    "def split_data(df):\n",
    "    # split data into train and test sets\n",
    "    seed      = SEED\n",
    "    val_size  = SPLIT_RATIOS[1]\n",
    "    test_size = SPLIT_RATIOS[2]\n",
    "    \n",
    "    num_samples = df.shape[0]\n",
    "    X1 = df.values[:num_samples, 1:] # keep only the features, skip the target, all rows\n",
    "    Y1 = df.values[:num_samples, :1] # keep only the target, all rows\n",
    "\n",
    "    # Use split ratios to divide up into train/val/test\n",
    "    X_train, X_val, y_train, y_val = \\\n",
    "        train_test_split(X1, Y1, test_size=(test_size + val_size), random_state=seed)\n",
    "    # Of the remaining non-training samples, give proper ratio to validation and to test\n",
    "    X_test, X_test, y_test, y_test = \\\n",
    "        train_test_split(X_val, y_val, test_size=(test_size / (test_size + val_size)), \n",
    "                         random_state=seed)\n",
    "    # reassemble the datasets with target in first column and features after that\n",
    "    _train = np.concatenate([y_train, X_train], axis=1)\n",
    "    _val   = np.concatenate([y_val,   X_val],   axis=1)\n",
    "    _test  = np.concatenate([y_test,  X_test],  axis=1)\n",
    "\n",
    "    return _train, _val, _test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch a single training job for a given housing location\n",
    "There is nothing specific to multi-model endpoints in terms of the models it will host. They are trained in the same way as all other SageMaker models. Here we are using the XGBoost estimator and not waiting for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_training_job(location):\n",
    "    # clear out old versions of the data\n",
    "    s3_bucket = s3.Bucket(BUCKET)\n",
    "    full_input_prefix = '{}/model_prep/{}'.format(DATA_PREFIX, location)\n",
    "    s3_bucket.objects.filter(Prefix=full_input_prefix + '/').delete()\n",
    "\n",
    "    # upload the entire set of data for all three channels\n",
    "    local_folder = 'data/{}'.format(location)\n",
    "    inputs = sagemaker_session.upload_data(path=local_folder, key_prefix=full_input_prefix)\n",
    "    print('Training data uploaded: {}'.format(inputs))\n",
    "    \n",
    "    _job = 'xgb-{}'.format(location.replace('_', '-'))\n",
    "    full_output_prefix = '{}/model_artifacts/{}'.format(DATA_PREFIX, location)\n",
    "    s3_output_path = 's3://{}/{}'.format(BUCKET, full_output_prefix)\n",
    "\n",
    "    xgb = sagemaker.estimator.Estimator(MULTI_MODEL_XGBOOST_IMAGE, role, \n",
    "                                        train_instance_count=1, train_instance_type=TRAIN_INSTANCE_TYPE,\n",
    "                                        output_path=s3_output_path, base_job_name=_job,\n",
    "                                        sagemaker_session=sagemaker_session)\n",
    "    xgb.set_hyperparameters(max_depth=5, eta=0.2, gamma=4, min_child_weight=6, subsample=0.8, silent=0, \n",
    "                            early_stopping_rounds=5, objective='reg:linear', num_round=25) \n",
    "    \n",
    "    DISTRIBUTION_MODE = 'FullyReplicated'\n",
    "    train_input = sagemaker.s3_input(s3_data=inputs+'/train', \n",
    "                                     distribution=DISTRIBUTION_MODE, content_type='csv')\n",
    "    val_input   = sagemaker.s3_input(s3_data=inputs+'/val', \n",
    "                                     distribution=DISTRIBUTION_MODE, content_type='csv')\n",
    "    remote_inputs = {'train': train_input, 'validation': val_input}\n",
    "\n",
    "    xgb.fit(remote_inputs, wait=False)\n",
    "    \n",
    "    return xgb.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kick off a model training job for each housing location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_locally(location, train, val, test):\n",
    "    os.makedirs('data/{}/train'.format(location))\n",
    "    np.savetxt( 'data/{0}/train/{0}_train.csv'.format(location), train, delimiter=',', fmt='%.2f')\n",
    "    \n",
    "    os.makedirs('data/{}/val'.format(location))\n",
    "    np.savetxt( 'data/{0}/val/{0}_val.csv'.format(location),     val, delimiter=',', fmt='%.2f')\n",
    "    \n",
    "    os.makedirs('data/{}/test'.format(location))\n",
    "    np.savetxt( 'data/{0}/test/{0}_test.csv'.format(location),   test, delimiter=',', fmt='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded: s3://sagemaker-us-east-2-111016121260/DEMO_MME_XGBOOST/model_prep/Boardman_Ohio\n",
      "Training data uploaded: s3://sagemaker-us-east-2-111016121260/DEMO_MME_XGBOOST/model_prep/Melrose_Massachusetts\n",
      "Training data uploaded: s3://sagemaker-us-east-2-111016121260/DEMO_MME_XGBOOST/model_prep/Belmont_California\n",
      "Training data uploaded: s3://sagemaker-us-east-2-111016121260/DEMO_MME_XGBOOST/model_prep/Bedford_Texas\n",
      "Training data uploaded: s3://sagemaker-us-east-2-111016121260/DEMO_MME_XGBOOST/model_prep/Louisville_Kentucky\n",
      "Training data uploaded: s3://sagemaker-us-east-2-111016121260/DEMO_MME_XGBOOST/model_prep/Bellingham_Washington\n",
      "Training data uploaded: s3://sagemaker-us-east-2-111016121260/DEMO_MME_XGBOOST/model_prep/Kailua_Hawaii\n",
      "Training data uploaded: s3://sagemaker-us-east-2-111016121260/DEMO_MME_XGBOOST/model_prep/MiddleRiver_Maryland\n",
      "Training data uploaded: s3://sagemaker-us-east-2-111016121260/DEMO_MME_XGBOOST/model_prep/Hawthorne_NewJersey\n",
      "Training data uploaded: s3://sagemaker-us-east-2-111016121260/DEMO_MME_XGBOOST/model_prep/EastPeoria_Illinois\n",
      "10 training jobs launched: ['xgb-Boardman-Ohio-2020-04-16-01-18-06-150', 'xgb-Melrose-Massachusetts-2020-04-16-01-18-06-529', 'xgb-Belmont-California-2020-04-16-01-18-08-182', 'xgb-Bedford-Texas-2020-04-16-01-18-09-087', 'xgb-Louisville-Kentucky-2020-04-16-01-18-09-984', 'xgb-Bellingham-Washington-2020-04-16-01-18-13-518', 'xgb-Kailua-Hawaii-2020-04-16-01-18-16-349', 'xgb-MiddleRiver-Maryland-2020-04-16-01-18-20-483', 'xgb-Hawthorne-NewJersey-2020-04-16-01-18-21-155', 'xgb-EastPeoria-Illinois-2020-04-16-01-18-21-991']\n",
      "Waiting for job xgb-Boardman-Ohio-2020-04-16-01-18-06-150 to complete...\n",
      "xgb-Boardman-Ohio-2020-04-16-01-18-06-150 job status: InProgress\n",
      "xgb-Boardman-Ohio-2020-04-16-01-18-06-150 job status: InProgress\n",
      "xgb-Boardman-Ohio-2020-04-16-01-18-06-150 job status: InProgress\n",
      "DONE. Status for xgb-Boardman-Ohio-2020-04-16-01-18-06-150 is Completed\n",
      "\n",
      "Waiting for job xgb-Melrose-Massachusetts-2020-04-16-01-18-06-529 to complete...\n",
      "DONE. Status for xgb-Melrose-Massachusetts-2020-04-16-01-18-06-529 is Completed\n",
      "\n",
      "Waiting for job xgb-Belmont-California-2020-04-16-01-18-08-182 to complete...\n",
      "DONE. Status for xgb-Belmont-California-2020-04-16-01-18-08-182 is Completed\n",
      "\n",
      "Waiting for job xgb-Bedford-Texas-2020-04-16-01-18-09-087 to complete...\n",
      "DONE. Status for xgb-Bedford-Texas-2020-04-16-01-18-09-087 is Completed\n",
      "\n",
      "Waiting for job xgb-Louisville-Kentucky-2020-04-16-01-18-09-984 to complete...\n",
      "DONE. Status for xgb-Louisville-Kentucky-2020-04-16-01-18-09-984 is Completed\n",
      "\n",
      "Waiting for job xgb-Bellingham-Washington-2020-04-16-01-18-13-518 to complete...\n",
      "DONE. Status for xgb-Bellingham-Washington-2020-04-16-01-18-13-518 is Completed\n",
      "\n",
      "Waiting for job xgb-Kailua-Hawaii-2020-04-16-01-18-16-349 to complete...\n",
      "DONE. Status for xgb-Kailua-Hawaii-2020-04-16-01-18-16-349 is Completed\n",
      "\n",
      "Waiting for job xgb-MiddleRiver-Maryland-2020-04-16-01-18-20-483 to complete...\n",
      "DONE. Status for xgb-MiddleRiver-Maryland-2020-04-16-01-18-20-483 is Completed\n",
      "\n",
      "Waiting for job xgb-Hawthorne-NewJersey-2020-04-16-01-18-21-155 to complete...\n",
      "DONE. Status for xgb-Hawthorne-NewJersey-2020-04-16-01-18-21-155 is Completed\n",
      "\n",
      "Waiting for job xgb-EastPeoria-Illinois-2020-04-16-01-18-21-991 to complete...\n",
      "DONE. Status for xgb-EastPeoria-Illinois-2020-04-16-01-18-21-991 is Completed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Yield successive n-sized \n",
    "# chunks from l. \n",
    "def divide_chunks(l, n): \n",
    "      \n",
    "    # looping till length l \n",
    "    for i in range(0, len(l), n):  \n",
    "        yield l[i:i + n] \n",
    "  \n",
    "# How many elements each \n",
    "# list should have \n",
    "n = 10\n",
    "  \n",
    "city_list_break = list(divide_chunks(LOCATIONS, n)) \n",
    "\n",
    "training_jobs_all = []\n",
    "\n",
    "\n",
    "for each in city_list_break:\n",
    "\n",
    "    def wait_for_training_job_to_complete(job_name):\n",
    "        print('Waiting for job {} to complete...'.format(job_name))\n",
    "        resp = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "        status = resp['TrainingJobStatus']\n",
    "        while status=='InProgress':\n",
    "            time.sleep(60)\n",
    "            resp = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "            status = resp['TrainingJobStatus']\n",
    "            if status == 'InProgress':\n",
    "                print('{} job status: {}'.format(job_name, status))\n",
    "        print('DONE. Status for {} is {}\\n'.format(job_name, status))\n",
    "\n",
    "    training_jobs = []\n",
    "\n",
    "    shutil.rmtree('data', ignore_errors=True)\n",
    "\n",
    "    for loc in each[:PARALLEL_TRAINING_JOBS]:\n",
    "        _houses = gen_houses(NUM_HOUSES_PER_LOCATION)\n",
    "        _train, _val, _test = split_data(_houses)\n",
    "        save_data_locally(loc, _train, _val, _test)\n",
    "        _job = launch_training_job(loc)\n",
    "        training_jobs.append(_job)\n",
    "        training_jobs_all.append(_job)\n",
    "    print('{} training jobs launched: {}'.format(len(training_jobs), training_jobs))\n",
    "\n",
    "\n",
    "    # wait for the jobs to finish\n",
    "    for j in training_jobs:\n",
    "        wait_for_training_job_to_complete(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for all model training to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_training_job_to_complete(job_name):\n",
    "    print('Waiting for job {} to complete...'.format(job_name))\n",
    "    resp = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "    status = resp['TrainingJobStatus']\n",
    "    while status=='InProgress':\n",
    "        time.sleep(60)\n",
    "        resp = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "        status = resp['TrainingJobStatus']\n",
    "        if status == 'InProgress':\n",
    "            print('{} job status: {}'.format(job_name, status))\n",
    "    print('DONE. Status for {} is {}\\n'.format(job_name, status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job xgb-Boardman-Ohio-2020-04-16-01-18-06-150 to complete...\n",
      "DONE. Status for xgb-Boardman-Ohio-2020-04-16-01-18-06-150 is Completed\n",
      "\n",
      "Waiting for job xgb-Melrose-Massachusetts-2020-04-16-01-18-06-529 to complete...\n",
      "DONE. Status for xgb-Melrose-Massachusetts-2020-04-16-01-18-06-529 is Completed\n",
      "\n",
      "Waiting for job xgb-Belmont-California-2020-04-16-01-18-08-182 to complete...\n",
      "DONE. Status for xgb-Belmont-California-2020-04-16-01-18-08-182 is Completed\n",
      "\n",
      "Waiting for job xgb-Bedford-Texas-2020-04-16-01-18-09-087 to complete...\n",
      "DONE. Status for xgb-Bedford-Texas-2020-04-16-01-18-09-087 is Completed\n",
      "\n",
      "Waiting for job xgb-Louisville-Kentucky-2020-04-16-01-18-09-984 to complete...\n",
      "DONE. Status for xgb-Louisville-Kentucky-2020-04-16-01-18-09-984 is Completed\n",
      "\n",
      "Waiting for job xgb-Bellingham-Washington-2020-04-16-01-18-13-518 to complete...\n",
      "DONE. Status for xgb-Bellingham-Washington-2020-04-16-01-18-13-518 is Completed\n",
      "\n",
      "Waiting for job xgb-Kailua-Hawaii-2020-04-16-01-18-16-349 to complete...\n",
      "DONE. Status for xgb-Kailua-Hawaii-2020-04-16-01-18-16-349 is Completed\n",
      "\n",
      "Waiting for job xgb-MiddleRiver-Maryland-2020-04-16-01-18-20-483 to complete...\n",
      "DONE. Status for xgb-MiddleRiver-Maryland-2020-04-16-01-18-20-483 is Completed\n",
      "\n",
      "Waiting for job xgb-Hawthorne-NewJersey-2020-04-16-01-18-21-155 to complete...\n",
      "DONE. Status for xgb-Hawthorne-NewJersey-2020-04-16-01-18-21-155 is Completed\n",
      "\n",
      "Waiting for job xgb-EastPeoria-Illinois-2020-04-16-01-18-21-991 to complete...\n",
      "DONE. Status for xgb-EastPeoria-Illinois-2020-04-16-01-18-21-991 is Completed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# wait for the jobs to finish\n",
    "for j in training_jobs:\n",
    "    wait_for_training_job_to_complete(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import models into hosting\n",
    "A big difference for multi-model endpoints is that when creating the Model entity, the container's `ModelDataUrl` is the S3 prefix where the model artifacts that are invokable by the endpoint are located. The rest of the S3 path will be specified when actually invoking the model. Remember to close the location with a trailing slash.\n",
    "\n",
    "The `Mode` of container is specified as `MultiModel` to signify that the container will host multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model artifacts to be found by the endpoint\n",
    "As described above, the multi-model endpoint is configured to find its model artifacts in a specific location in S3. For each trained model, we make a copy of its model artifacts into that location.\n",
    "\n",
    "In our example, we are storing all the models within a single folder. The implementation of multi-model endpoints is flexible enough to permit an arbitrary folder structure. For a set of housing models for example, you could have a top level folder for each region, and the model artifacts would be copied to those regional folders. The target model referenced when invoking such a model would include the folder path. For example, `northeast/Boston_MA.tar.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_model_artifacts(model_data_url):\n",
    "    # extract the s3 key from the full url to the model artifacts\n",
    "    _s3_key = model_data_url.split('s3://{}/'.format(BUCKET))[1]\n",
    "    # get the part of the key that identifies the model within the model artifacts folder\n",
    "    _model_name_plus = _s3_key[_s3_key.find('model_artifacts') + len('model_artifacts') + 1:]\n",
    "    # finally, get the unique model name (e.g., \"NewYork_NY\")\n",
    "    _model_name = re.findall('^(.*?)/', _model_name_plus)[0]\n",
    "    return _s3_key, _model_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the model artifacts from the original output of the training job to the place in\n",
    "# s3 where the multi model endpoint will dynamically load individual models\n",
    "def deploy_artifacts_to_mme(job_name):\n",
    "    _resp = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "    _source_s3_key, _model_name = parse_model_artifacts(_resp['ModelArtifacts']['S3ModelArtifacts'])\n",
    "    _copy_source = {'Bucket': BUCKET, 'Key': _source_s3_key}\n",
    "    _key = '{}/{}/{}.tar.gz'.format(DATA_PREFIX, MULTI_MODEL_ARTIFACTS, _model_name)\n",
    "    \n",
    "    print('Copying {} model\\n   from: {}\\n     to: {}...'.format(_model_name, _source_s3_key, _key))\n",
    "    s3_client.copy_object(Bucket=BUCKET, CopySource=_copy_source, Key=_key)\n",
    "    return _key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are purposely *not* copying the first model. This will be copied later in the notebook to demonstrate how to dynamically add new models to an already running endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing old model artifacts from DEMO_MME_XGBOOST/multi_model_artifacts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'ResponseMetadata': {'RequestId': '982D83D244B70CFE',\n",
       "   'HostId': '8k9lc+z1aSGNk9Z3CdnkLJa/9BVkDOH0DVnJ6H1gONq66+c+9mhsJaI9yWatRGJ6+UoMNBYIxHw=',\n",
       "   'HTTPStatusCode': 200,\n",
       "   'HTTPHeaders': {'x-amz-id-2': '8k9lc+z1aSGNk9Z3CdnkLJa/9BVkDOH0DVnJ6H1gONq66+c+9mhsJaI9yWatRGJ6+UoMNBYIxHw=',\n",
       "    'x-amz-request-id': '982D83D244B70CFE',\n",
       "    'date': 'Thu, 16 Apr 2020 01:23:34 GMT',\n",
       "    'connection': 'close',\n",
       "    'content-type': 'application/xml',\n",
       "    'transfer-encoding': 'chunked',\n",
       "    'server': 'AmazonS3'},\n",
       "   'RetryAttempts': 0},\n",
       "  'Deleted': [{'Key': 'DEMO_MME_XGBOOST/multi_model_artifacts/Edgewater_Florida.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_XGBOOST/multi_model_artifacts/Akron_Ohio.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_XGBOOST/multi_model_artifacts/Riverside_Ohio.tar.gz'},\n",
       "   {'Key': 'DEMO_MME_XGBOOST/multi_model_artifacts/BayshoreGardens_Florida.tar.gz'}]}]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, clear out old versions of the model artifacts from previous runs of this notebook\n",
    "s3 = boto3.resource('s3')\n",
    "s3_bucket = s3.Bucket(BUCKET)\n",
    "full_input_prefix = '{}/multi_model_artifacts'.format(DATA_PREFIX)\n",
    "print('Removing old model artifacts from {}'.format(full_input_prefix))\n",
    "s3_bucket.objects.filter(Prefix=full_input_prefix + '/').delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying Melrose_Massachusetts model\n",
      "   from: DEMO_MME_XGBOOST/model_artifacts/Melrose_Massachusetts/xgb-Melrose-Massachusetts-2020-04-16-01-18-06-529/output/model.tar.gz\n",
      "     to: DEMO_MME_XGBOOST/multi_model_artifacts/Melrose_Massachusetts.tar.gz...\n",
      "Copying Belmont_California model\n",
      "   from: DEMO_MME_XGBOOST/model_artifacts/Belmont_California/xgb-Belmont-California-2020-04-16-01-18-08-182/output/model.tar.gz\n",
      "     to: DEMO_MME_XGBOOST/multi_model_artifacts/Belmont_California.tar.gz...\n",
      "Copying Bedford_Texas model\n",
      "   from: DEMO_MME_XGBOOST/model_artifacts/Bedford_Texas/xgb-Bedford-Texas-2020-04-16-01-18-09-087/output/model.tar.gz\n",
      "     to: DEMO_MME_XGBOOST/multi_model_artifacts/Bedford_Texas.tar.gz...\n",
      "Copying Louisville_Kentucky model\n",
      "   from: DEMO_MME_XGBOOST/model_artifacts/Louisville_Kentucky/xgb-Louisville-Kentucky-2020-04-16-01-18-09-984/output/model.tar.gz\n",
      "     to: DEMO_MME_XGBOOST/multi_model_artifacts/Louisville_Kentucky.tar.gz...\n",
      "Copying Bellingham_Washington model\n",
      "   from: DEMO_MME_XGBOOST/model_artifacts/Bellingham_Washington/xgb-Bellingham-Washington-2020-04-16-01-18-13-518/output/model.tar.gz\n",
      "     to: DEMO_MME_XGBOOST/multi_model_artifacts/Bellingham_Washington.tar.gz...\n",
      "Copying Kailua_Hawaii model\n",
      "   from: DEMO_MME_XGBOOST/model_artifacts/Kailua_Hawaii/xgb-Kailua-Hawaii-2020-04-16-01-18-16-349/output/model.tar.gz\n",
      "     to: DEMO_MME_XGBOOST/multi_model_artifacts/Kailua_Hawaii.tar.gz...\n",
      "Copying MiddleRiver_Maryland model\n",
      "   from: DEMO_MME_XGBOOST/model_artifacts/MiddleRiver_Maryland/xgb-MiddleRiver-Maryland-2020-04-16-01-18-20-483/output/model.tar.gz\n",
      "     to: DEMO_MME_XGBOOST/multi_model_artifacts/MiddleRiver_Maryland.tar.gz...\n",
      "Copying Hawthorne_NewJersey model\n",
      "   from: DEMO_MME_XGBOOST/model_artifacts/Hawthorne_NewJersey/xgb-Hawthorne-NewJersey-2020-04-16-01-18-21-155/output/model.tar.gz\n",
      "     to: DEMO_MME_XGBOOST/multi_model_artifacts/Hawthorne_NewJersey.tar.gz...\n",
      "Copying EastPeoria_Illinois model\n",
      "   from: DEMO_MME_XGBOOST/model_artifacts/EastPeoria_Illinois/xgb-EastPeoria-Illinois-2020-04-16-01-18-21-991/output/model.tar.gz\n",
      "     to: DEMO_MME_XGBOOST/multi_model_artifacts/EastPeoria_Illinois.tar.gz...\n"
     ]
    }
   ],
   "source": [
    "# copy every model except the first one\n",
    "for job in training_jobs_all[1:]:\n",
    "    deploy_artifacts_to_mme(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Amazon SageMaker model entity\n",
    "Here we use `boto3` to create the model entity. Instead of describing a single model, it will indicate the use of multi-model semantics and will identify the source location of all specific model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_model_entity(multi_model_name, role):\n",
    "    # establish the place in S3 from which the endpoint will pull individual models\n",
    "    _model_url  = 's3://{}/{}/{}/'.format(BUCKET, DATA_PREFIX, MULTI_MODEL_ARTIFACTS)\n",
    "    _container = {\n",
    "        'Image':        MULTI_MODEL_XGBOOST_IMAGE,\n",
    "        'ModelDataUrl': _model_url,\n",
    "        'Mode':         'MultiModel'\n",
    "    }\n",
    "    create_model_response = sm_client.create_model(\n",
    "        ModelName = multi_model_name,\n",
    "        ExecutionRoleArn = role,\n",
    "        Containers = [_container])\n",
    "    \n",
    "    return _model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi model name: housing-2020-04-16-01-23-41\n"
     ]
    }
   ],
   "source": [
    "multi_model_name = '{}-{}'.format(HOUSING_MODEL_NAME, strftime('%Y-%m-%d-%H-%M-%S', gmtime()))\n",
    "model_url = create_multi_model_entity(multi_model_name, role)\n",
    "print('Multi model name: {}'.format(multi_model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the multi-model endpoint\n",
    "There is nothing special about the SageMaker endpoint config for a multi-model endpoint. You need to consider the appropriate instance type and number of instances for the projected prediction workload. The number and size of the individual models will drive memory requirements.\n",
    "\n",
    "Once the endpoint config is in place, the endpoint creation is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint config name: housing-2020-04-16-01-23-41\n",
      "Endpoint name: housing-2020-04-16-01-23-41\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = multi_model_name\n",
    "print('Endpoint config name: ' + endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': ENDPOINT_INSTANCE_TYPE,\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'ModelName': multi_model_name,\n",
    "        'VariantName': 'AllTraffic'}])\n",
    "\n",
    "endpoint_name = multi_model_name\n",
    "print('Endpoint name: ' + endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-east-2:111016121260:endpoint/housing-2020-04-16-01-23-41\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print('Endpoint Arn: ' + create_endpoint_response['EndpointArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for housing-2020-04-16-01-23-41 endpoint to be in service...\n"
     ]
    }
   ],
   "source": [
    "print('Waiting for {} endpoint to be in service...'.format(endpoint_name))\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise the multi-model endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke multiple individual models hosted behind a single endpoint\n",
    "Here we iterate through a set of housing predictions, choosing the specific location-based housing model at random. Notice the cold start price paid for the first invocation of any given model. Subsequent invocations of the same model take advantage of the model already being loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_house_value(features, model_name):\n",
    "    print('Using model {} to predict price of this house: {}'.format(full_model_name,\n",
    "                                                                     features))\n",
    "    body = ','.join(map(str, features)) + '\\n'\n",
    "    start_time = time.time()\n",
    "\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "                        EndpointName=endpoint_name,\n",
    "                        ContentType='text/csv',\n",
    "                        TargetModel=full_model_name,\n",
    "                        Body=body)\n",
    "    predicted_value = json.loads(response['Body'].read())[0]\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    print('${:,.2f}, took {:,d} ms\\n'.format(predicted_value, int(duration * 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the models that the endpoint has at its disposal:\n",
      "2020-04-16 01:23:35   11.5 KiB Bedford_Texas.tar.gz\n",
      "2020-04-16 01:23:35   11.1 KiB Bellingham_Washington.tar.gz\n",
      "2020-04-16 01:23:35   11.2 KiB Belmont_California.tar.gz\n",
      "2020-04-16 01:23:36   11.3 KiB EastPeoria_Illinois.tar.gz\n",
      "2020-04-16 01:23:36   11.5 KiB Hawthorne_NewJersey.tar.gz\n",
      "2020-04-16 01:23:35   10.8 KiB Kailua_Hawaii.tar.gz\n",
      "2020-04-16 01:23:35   10.7 KiB Louisville_Kentucky.tar.gz\n",
      "2020-04-16 01:23:35   11.5 KiB Melrose_Massachusetts.tar.gz\n",
      "2020-04-16 01:23:35   10.8 KiB MiddleRiver_Maryland.tar.gz\n",
      "\n",
      "Total Objects: 9\n",
      "   Total Size: 100.5 KiB\n"
     ]
    }
   ],
   "source": [
    "print('Here are the models that the endpoint has at its disposal:')\n",
    "!aws s3 ls --human-readable --summarize $model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model MiddleRiver_Maryland.tar.gz to predict price of this house: [1995, 2923, 6, 3.0, 0.94, 2]\n",
      "$449,660.94, took 1,182 ms\n",
      "\n",
      "Using model Kailua_Hawaii.tar.gz to predict price of this house: [1984, 2470, 3, 2.5, 1.05, 2]\n",
      "$317,114.06, took 670 ms\n",
      "\n",
      "Using model Kailua_Hawaii.tar.gz to predict price of this house: [1981, 1799, 4, 1.0, 1.14, 2]\n",
      "$217,920.53, took 17 ms\n",
      "\n",
      "Using model MiddleRiver_Maryland.tar.gz to predict price of this house: [2007, 2929, 5, 2.0, 1.21, 1]\n",
      "$484,644.28, took 16 ms\n",
      "\n",
      "Using model Melrose_Massachusetts.tar.gz to predict price of this house: [1970, 4033, 4, 2.0, 1.57, 2]\n",
      "$533,263.12, took 690 ms\n",
      "\n",
      "Using model Hawthorne_NewJersey.tar.gz to predict price of this house: [1983, 2535, 5, 3.0, 0.71, 2]\n",
      "$326,593.00, took 647 ms\n",
      "\n",
      "Using model Kailua_Hawaii.tar.gz to predict price of this house: [1994, 3351, 2, 2.0, 0.9, 2]\n",
      "$493,389.38, took 15 ms\n",
      "\n",
      "Using model MiddleRiver_Maryland.tar.gz to predict price of this house: [2005, 3897, 5, 1.0, 1.31, 3]\n",
      "$652,086.56, took 15 ms\n",
      "\n",
      "Using model Melrose_Massachusetts.tar.gz to predict price of this house: [1993, 2544, 2, 2.0, 0.88, 3]\n",
      "$359,140.31, took 18 ms\n",
      "\n",
      "Using model MiddleRiver_Maryland.tar.gz to predict price of this house: [1990, 2132, 4, 2.0, 0.45, 2]\n",
      "$287,543.62, took 15 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iterate through invocations with random inputs against a random model showing results and latency\n",
    "for i in range(10):\n",
    "    model_name = LOCATIONS[np.random.randint(1, len(LOCATIONS[:PARALLEL_TRAINING_JOBS]))]\n",
    "    full_model_name = '{}.tar.gz'.format(model_name)\n",
    "    predict_one_house_value(gen_random_house()[1:], full_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamically deploy another model\n",
    "Here we demonstrate the power of dynamic loading of new models. We purposely did not copy the first model when deploying models earlier. Now we deploy an additional model and can immediately invoke it through the multi-model endpoint. As with the earlier models, the first invocation to the new model takes longer, as the endpoint takes time to download the model and load it into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying Boardman_Ohio model\n",
      "   from: DEMO_MME_XGBOOST/model_artifacts/Boardman_Ohio/xgb-Boardman-Ohio-2020-04-16-01-18-06-150/output/model.tar.gz\n",
      "     to: DEMO_MME_XGBOOST/multi_model_artifacts/Boardman_Ohio.tar.gz...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DEMO_MME_XGBOOST/multi_model_artifacts/Boardman_Ohio.tar.gz'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add another model to the endpoint and exercise it\n",
    "deploy_artifacts_to_mme(training_jobs_all[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the newly deployed model\n",
    "Exercise the newly deployed model without the need for any endpoint update or restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the models that the endpoint has at its disposal:\n",
      "2020-04-16 01:23:35      11785 Bedford_Texas.tar.gz\n",
      "2020-04-16 01:23:35      11340 Bellingham_Washington.tar.gz\n",
      "2020-04-16 01:23:35      11494 Belmont_California.tar.gz\n",
      "2020-04-16 01:30:49      11453 Boardman_Ohio.tar.gz\n",
      "2020-04-16 01:23:36      11615 EastPeoria_Illinois.tar.gz\n",
      "2020-04-16 01:23:36      11768 Hawthorne_NewJersey.tar.gz\n",
      "2020-04-16 01:23:35      11028 Kailua_Hawaii.tar.gz\n",
      "2020-04-16 01:23:35      10990 Louisville_Kentucky.tar.gz\n",
      "2020-04-16 01:23:35      11812 Melrose_Massachusetts.tar.gz\n",
      "2020-04-16 01:23:35      11043 MiddleRiver_Maryland.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print('Here are the models that the endpoint has at its disposal:')\n",
    "!aws s3 ls $model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model Boardman_Ohio.tar.gz to predict price of this house: [1991, 4135, 5, 3.0, 1.15, 0]\n",
      "$584,386.31, took 679 ms\n",
      "\n",
      "Using model Boardman_Ohio.tar.gz to predict price of this house: [1993, 3172, 5, 2.0, 0.96, 3]\n",
      "$467,577.41, took 17 ms\n",
      "\n",
      "Using model Boardman_Ohio.tar.gz to predict price of this house: [2003, 2420, 2, 1.0, 0.89, 2]\n",
      "$371,897.81, took 12 ms\n",
      "\n",
      "Using model Boardman_Ohio.tar.gz to predict price of this house: [1999, 3874, 4, 2.0, 0.6, 0]\n",
      "$566,508.06, took 12 ms\n",
      "\n",
      "Using model Boardman_Ohio.tar.gz to predict price of this house: [1981, 3454, 6, 1.5, 1.45, 3]\n",
      "$487,151.31, took 13 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = LOCATIONS[0]\n",
    "full_model_name = '{}.tar.gz'.format(model_name)\n",
    "for i in range(5):\n",
    "    features = gen_random_house()\n",
    "    predict_one_house_value(gen_random_house()[1:], full_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating a model\n",
    "To update a model, you would follow the same approach as above and add it as a new model. For example, if you have retrained the `NewYork_NY.tar.gz` model and wanted to start invoking it, you would upload the updated model artifacts behind the S3 prefix with a new name such as `NewYork_NY_v2.tar.gz`, and then change the `TargetModel` field to invoke `NewYork_NY_v2.tar.gz` instead of `NewYork_NY.tar.gz`. You do not want to overwrite the model artifacts in Amazon S3, because the old version of the model might still be loaded in the containers or on the storage volume of the instances on the endpoint. Invocations to the new model could then invoke the old version of the model.\n",
    "\n",
    "Alternatively, you could stop the endpoint and re-deploy a fresh set of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "Here, to be sure we are not billed for endpoints we are no longer using, we clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'ca16203f-36f7-4cf0-81fc-0b25fe2c6fb6',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'ca16203f-36f7-4cf0-81fc-0b25fe2c6fb6',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Thu, 16 Apr 2020 01:31:10 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shut down the endpoint\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'ea9dc9c7-6412-4346-a8e9-79032387dd4c',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'ea9dc9c7-6412-4346-a8e9-79032387dd4c',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Thu, 16 Apr 2020 01:31:10 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the endpoint config\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'c8c48dbd-2040-4376-8d78-cf420db90829',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'c8c48dbd-2040-4376-8d78-cf420db90829',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Thu, 16 Apr 2020 01:31:11 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete model too\n",
    "sm_client.delete_model(ModelName=multi_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
